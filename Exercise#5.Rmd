---
title: "Exercise 5: Penalized Regression with Hitters Dataset"
author: "Aldair Leon"
output: html_document
---

```{r setup, include=FALSE}
# Configure knitr to show R code in output
knitr::opts_chunk$set(echo = TRUE)
```

## 5(a) Load and Clean the Dataset

```{r}
  # Load required libraries
  library(tidyverse)
  library(glmnet)
  
  # Ensure reproducibility
  set.seed(1122)
  
  # Load Hitters dataset
  
  setwd(dirname(rstudioapi::getSourceEditorContext()$path))
  abs_path_dataset <- file.path(setwd(dirname(rstudioapi::getSourceEditorContext()$path)), "data","Hitters.csv")
  hitters <- read.csv(abs_path_dataset, sep = " ")
  
  # Remove rows with missing values
  hitters_clean <- hitters[complete.cases(hitters), ]
```

## 5(b) Compute Condition Number of XtX

```{r}
# Create design matrix X and response vector y
X <- model.matrix(Salary ~ ., hitters_clean)[, -1]  # Remove intercept column
y <- hitters_clean$Salary

# Compute XtX matrix and condition number
XtX <- t(X) %*% X

# Eigenvalue decomposition to get condition number
eigenvalues <- eigen(XtX)$values
condition_number <- max(eigenvalues) / min(eigenvalues)

# Standardize X to improve conditioning
X_scaled <- scale(X)
XtX_scaled <- t(X_scaled) %*% X_scaled

eigen_scaled <- eigen(XtX_scaled)$values
condition_scaled <- max(eigen_scaled) / min(eigen_scaled)

# Return both condition numbers
condition_number
condition_scaled
```

## 5(c) Compare OLS vs Ridge Regression (Î» = 70)

```{r}
# Fit ordinary least squares regression
lm_fit <- lm(Salary ~ ., data = hitters_clean)

# Fit Ridge regression with fixed lambda
ridge_fit <- glmnet(X_scaled, y, alpha = 0, lambda = 70)

# Extract coefficients (excluding intercepts)
coef_lm <- coef(lm_fit)[-1]
coef_ridge <- as.vector(coef(ridge_fit))[-1]

# Combine for comparison
df_compare <- tibble(
  Variable = colnames(X),
  LM_Coeff = coef_lm,
  Ridge_Coeff = coef_ridge
)

# Show side-by-side coefficients
df_compare
```

## 5(d) Train-Test Split

```{r}
# Determine training set size (70%)
n <- nrow(hitters_clean)
train_idx <- sample(1:n, size = floor(0.7 * n))

# Split scaled X and y into train/test
X_train <- X_scaled[train_idx, ]
y_train <- y[train_idx]
X_test <- X_scaled[-train_idx, ]
y_test <- y[-train_idx]
```

## 5(e) Ridge: MSE vs Lambda

```{r}
# Function to compute Ridge test MSE for a given lambda
ridge_mse <- function(lambda) {
  fit <- glmnet(X_train, y_train, alpha = 0, lambda = lambda)
  preds <- predict(fit, newx = X_test)
  mean((y_test - preds)^2)
}

# Generate lambda grid and compute MSE
lambda_grid <- 10^seq(10, -2, length.out = 100)
mse_values <- sapply(lambda_grid, ridge_mse)

# Plot MSE vs log(lambda)
plot(log(lambda_grid), mse_values, type = "l", col = "blue",
     xlab = "log(Lambda)", ylab = "Test MSE", main = "Ridge: MSE vs Lambda")

# Select lambda with minimum MSE
lambda_opt <- lambda_grid[which.min(mse_values)]
lambda_opt
```

## 5(f) Final Ridge Model Coefficients

```{r}
# Fit Ridge model on full data using optimal lambda
final_ridge <- glmnet(X_scaled, y, alpha = 0, lambda = lambda_opt)

# Extract non-zero coefficients
ridge_coefs <- coef(final_ridge)
ridge_coefs
```

## 5(g) Lasso Regression

```{r}
# Function to compute Lasso test MSE for a given lambda
lasso_mse <- function(lambda) {
  fit <- glmnet(X_train, y_train, alpha = 1, lambda = lambda)
  preds <- predict(fit, newx = X_test)
  mean((y_test - preds)^2)
}

# Compute MSE across lambda grid for Lasso
mse_lasso <- sapply(lambda_grid, lasso_mse)

# Plot MSE vs log(lambda)
plot(log(lambda_grid), mse_lasso, type = "l", col = "red",
     xlab = "log(Lambda)", ylab = "Test MSE", main = "Lasso: MSE vs Lambda")

# Select optimal lambda for Lasso
lambda_opt_lasso <- lambda_grid[which.min(mse_lasso)]
lambda_opt_lasso

# Fit Lasso model on full data using optimal lambda
final_lasso <- glmnet(X_scaled, y, alpha = 1, lambda = lambda_opt_lasso)

# Extract non-zero coefficients
lasso_coefs <- coef(final_lasso)
lasso_coefs
```
