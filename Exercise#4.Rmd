---
title: "Linear Regression Analysis on House Prices"
output: html_document
author: "Aldair Leon"
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(caret)
library(broom)
library(gridExtra)
```

### Load and Prepare Data

```{r load-data}
# Load dataset
source('scripts/abs_path.r', chdir = TRUE)
abs_path_dataset <- path_data("House prices.csv")
data <- read.csv(abs_path_dataset)

# Select relevant variables for modeling
data <- data %>% 
  select(price, bedrooms, bathrooms, sqft_living, floors, view, condition, grade, yr_built)

# Remove missing values to avoid model errors
data <- na.omit(data)
```

### (a) Linear Regression Model with Original Price

1.  The results of the linear regression model indicate that all the selected variables are statistically significant predictors of house price. This is supported by the small p-values (all below 0.001), showing strong evidence that each variable contributes meaningfully to the model. The coefficient for bedrooms is negative, which might seem counter intuitive at first but could be due to overlap with other variables (like square footage). In contrast, variables such as bathrooms, sqft_living, floors, view, condition, and grade all have positive effects on the price, meaning that an increase in any of these tends to increase the predicted price. Interestingly, yr_built has a negative coefficient, suggesting that newer houses (with higher years) are associated with higher prices. The model explains approximately 63.6% of the variation in house prices (RÂ² = 0.6359), which is considered strong.

2.  First, the Residuals vs Fitted and Scale-Location plots show a clear pattern where the spread of the residuals increases with the fitted values. This indicates heteroscedasticity, meaning the variance of the errors is not constant across all levels of the predicted values, therefore, a violation of one of the key linear regression assumptions. Second, the Q-Q plot suggests that the residuals are not normally distributed, particularly in the tails, which could affect the reliability of confidence intervals and hypothesis tests. Lastly, the Residuals vs Leverage plot shows a few observations with unusually high leverage and influence, suggesting that these outliers may be distorting the results of the model.

```{r lm-original}
# Fit linear regression using original price
lm1 <- lm(price ~ ., data = data)
summary(lm1)  # Output model summary

# Check regression assumptions via residual plots
par(mfrow = c(2, 2))
plot(lm1)
```

### (b) Log-transform Price and Compare

2)  The transformation improved the distribution of residuals and helped stabilize the variance. The histogram and Q-Q plot of log(price) indicate a distribution much closer to normality compared to the original price variable, which was strongly right-skewed. Additionally, the residual diagnostic plots for the log-transformed model show less heteroscedasticity and more symmetrical residuals. The R-squared of the log-transformed model (0.6426) is slightly higher than the original model (0.6359), indicating a improvement in explanatory power. Furthermore, all variables remain statistically significant, suggesting that the transformation improves model. Overall, the model using log(price) provides a better fit and satisfies the assumptions of linear regression more effectively than the original model.

```{r lm-log-price}
# Visualize distribution of original and log-transformed price
p1 <- ggplot(data, aes(x = price)) + 
  geom_histogram(bins = 50, fill = "steelblue") + 
  ggtitle("Histogram of Price")

p2 <- ggplot(data, aes(sample = price)) + 
  stat_qq() + stat_qq_line() + 
  ggtitle("QQ-Plot of Price")

p3 <- ggplot(data, aes(x = log(price))) + 
  geom_histogram(bins = 50, fill = "darkgreen") + 
  ggtitle("Histogram of log(Price)")

p4 <- ggplot(data, aes(sample = log(price))) + 
  stat_qq() + stat_qq_line() + 
  ggtitle("QQ-Plot of log(Price)")

# Arrange plots in grid layout
grid.arrange(p1, p2, p3, p4, ncol = 2)

# Add log-transformed price to dataset
data$log_price <- log(data$price)

# Fit linear model using log(price)
lm2 <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = data)
summary(lm2)

# Diagnostic plots
par(mfrow = c(2, 2))
plot(lm2)
```

### (c) Model with squared terms

1.  Both squared terms are highly statistically significant (p \< 2e-16), suggesting that the relationship between house price and these predictors is not only linear. The inclusion of these variables leads to a increase in model fit, with the R-squared improving from 0.6426 to 0.6492.

```{r lm-squared}
# Create squared terms to capture non-linear effects
data$sq_yr_built <- data$yr_built^2
data$sq_sqft_living <- data$sqft_living^2

# Fit model with squared terms
lm3 <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built + sq_yr_built + sq_sqft_living, data = data)
summary(lm3)

# Plot residuals to assess model fit
par(mfrow = c(2, 2))
plot(lm3)
```

### (d) Train-Test Split and Prediction

```{r train-test}
# Reproducibility of random sampling
set.seed(1122)

# Create training and testing indices (80-20 split)
train_index <- sample(1:nrow(data), 10806)
train <- data[train_index, ]
test <- data[-train_index, ]

# Model from (b): basic predictors
model_b <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built, data = train)
pred_b <- predict(model_b, newdata = test)
mse_b <- mean((pred_b - test$log_price)^2)  # Mean Squared Error

# Model from (c): extended with squared features
model_c <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + view + condition + grade + yr_built + sq_yr_built + sq_sqft_living, data = train)
pred_c <- predict(model_c, newdata = test)
mse_c <- mean((pred_c - test$log_price)^2)  # Mean Squared Error

# Output MSEs for comparison
mse_b; mse_c
```
